{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/ahmad.rammal/miniconda3/envs/colab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pruning import *\n",
    "from train_utils import *\n",
    "import json\n",
    "import copy \n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate all the acceptable model sizes for the GPT-2\n",
    "\n",
    "# num_heads_options = [8, 10, 12]\n",
    "# hidden_size_options = [2.5, 3, 3.5, 4]\n",
    "# embed_size_options = [512, 640, 768]\n",
    "\n",
    "# param_range = (115_000_000, 135_000_000)\n",
    "\n",
    "# model_name = \"openai-community/gpt2-medium\"\n",
    "# base_model, tokenizer = load_model(model_name)\n",
    "\n",
    "# acceptable_params = find_acceptable_model_sizes(base_model, tokenizer, num_heads_options, hidden_size_options, embed_size_options, param_range)\n",
    "\n",
    "# Load params directly from file\n",
    "with open(\"pruning_params.json\", \"r\") as f:\n",
    "    acceptable_params = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Do forward pass \n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", trust_remote_code=True)\n",
    "model_name = \"openai-community/gpt2-medium\"\n",
    "base_model, tokenizer = load_model(model_name)\n",
    "calibration_pass(model=base_model,\n",
    "                 tokenizer=tokenizer,\n",
    "                 dataset=dataset,\n",
    "                 sample_size=128,\n",
    "                 batch_size=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'GPT2TokenizerFast' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saved_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m training_metrics_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saved_metrics/training_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/pruned_GPT/train_utils.py:66\u001b[0m, in \u001b[0;36mtokenize_dataset\u001b[0;34m(tokenizer, dataset)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_dataset\u001b[39m(tokenizer, dataset):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes the dataset using the tokenizer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer(\n\u001b[1;32m     70\u001b[0m             examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     71\u001b[0m             truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     73\u001b[0m         )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GPT2TokenizerFast' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_dataset(tokenizer, dataset)\n",
    "\n",
    "os.makedirs(\"./saved_metrics\", exist_ok=True)\n",
    "\n",
    "training_metrics_path = \"./saved_metrics/training_metrics.json\"\n",
    "eval_metrics_path = \"./saved_metrics/eval_metrics.json\"\n",
    "\n",
    "if os.path.exists(training_metrics_path):\n",
    "    with open(training_metrics_path, \"r\") as f:\n",
    "        training_metrics = json.load(f)\n",
    "else:\n",
    "    training_metrics = {}\n",
    "\n",
    "if os.path.exists(eval_metrics_path):\n",
    "    with open(eval_metrics_path, \"r\") as f:\n",
    "        eval_metrics = json.load(f)\n",
    "else:\n",
    "    eval_metrics = {}\n",
    "\n",
    "for param in acceptable_params:\n",
    "    num_heads = param[\"num_heads\"]\n",
    "    hidden_size = param[\"hidden_size\"]\n",
    "    embed_size = param[\"embed_size\"]\n",
    "\n",
    "    param_key = f\"num_heads={num_heads}_hidden_size={hidden_size}_embed_size={embed_size}\"\n",
    "\n",
    "    if param_key in training_metrics:\n",
    "        print(f\"Skipping training for {param_key}, already exists.\")\n",
    "    else:\n",
    "        print(f\"Training model for {param_key}...\")\n",
    "        \n",
    "        model = copy.deepcopy(base_model)\n",
    "\n",
    "        prune_model(model, num_heads, int(hidden_size * embed_size), embed_size)\n",
    "\n",
    "        remove_all_forward_hooks(model)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        trainer = trainer_gpt2(model, tokenizer, tokenized_dataset, batch_size=4, num_epochs=2)\n",
    "        trainer.evaluate()\n",
    "        trainer.train()\n",
    "        training_metrics[param_key] = trainer.log_metrics()\n",
    "\n",
    "        with open(training_metrics_path, \"w\") as f:\n",
    "            json.dump(training_metrics, f, indent=4)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if param_key in eval_metrics:\n",
    "        print(f\"Skipping evaluation for {param_key}, already exists.\")\n",
    "    else:\n",
    "        print(f\"Evaluating perplexity for {param_key}...\")\n",
    "\n",
    "        model = copy.deepcopy(base_model)\n",
    "        eval_metrics[param_key] = evaluate_perplexity(model, tokenizer, stride=1024)\n",
    "\n",
    "        with open(eval_metrics_path, \"w\") as f:\n",
    "            json.dump(eval_metrics, f, indent=4)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/ahmad.rammal/miniconda3/envs/colab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Force float16 instead of BF16\n",
    "    device_map=\"auto\"           # Auto-detect the best device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 1024])\n",
      "transformer.wpe.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.ln_1.weight torch.Size([1024])\n",
      "transformer.h.0.ln_1.bias torch.Size([1024])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.0.ln_2.weight torch.Size([1024])\n",
      "transformer.h.0.ln_2.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.1.ln_1.weight torch.Size([1024])\n",
      "transformer.h.1.ln_1.bias torch.Size([1024])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.1.ln_2.weight torch.Size([1024])\n",
      "transformer.h.1.ln_2.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.2.ln_1.weight torch.Size([1024])\n",
      "transformer.h.2.ln_1.bias torch.Size([1024])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.2.ln_2.weight torch.Size([1024])\n",
      "transformer.h.2.ln_2.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.3.ln_1.weight torch.Size([1024])\n",
      "transformer.h.3.ln_1.bias torch.Size([1024])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.3.ln_2.weight torch.Size([1024])\n",
      "transformer.h.3.ln_2.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.4.ln_1.weight torch.Size([1024])\n",
      "transformer.h.4.ln_1.bias torch.Size([1024])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.4.ln_2.weight torch.Size([1024])\n",
      "transformer.h.4.ln_2.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.5.ln_1.weight torch.Size([1024])\n",
      "transformer.h.5.ln_1.bias torch.Size([1024])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.5.ln_2.weight torch.Size([1024])\n",
      "transformer.h.5.ln_2.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.6.ln_1.weight torch.Size([1024])\n",
      "transformer.h.6.ln_1.bias torch.Size([1024])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.6.ln_2.weight torch.Size([1024])\n",
      "transformer.h.6.ln_2.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.7.ln_1.weight torch.Size([1024])\n",
      "transformer.h.7.ln_1.bias torch.Size([1024])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.7.ln_2.weight torch.Size([1024])\n",
      "transformer.h.7.ln_2.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.8.ln_1.weight torch.Size([1024])\n",
      "transformer.h.8.ln_1.bias torch.Size([1024])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.8.ln_2.weight torch.Size([1024])\n",
      "transformer.h.8.ln_2.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.9.ln_1.weight torch.Size([1024])\n",
      "transformer.h.9.ln_1.bias torch.Size([1024])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.9.ln_2.weight torch.Size([1024])\n",
      "transformer.h.9.ln_2.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.10.ln_1.weight torch.Size([1024])\n",
      "transformer.h.10.ln_1.bias torch.Size([1024])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.10.ln_2.weight torch.Size([1024])\n",
      "transformer.h.10.ln_2.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.11.ln_1.weight torch.Size([1024])\n",
      "transformer.h.11.ln_1.bias torch.Size([1024])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.11.ln_2.weight torch.Size([1024])\n",
      "transformer.h.11.ln_2.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.12.ln_1.weight torch.Size([1024])\n",
      "transformer.h.12.ln_1.bias torch.Size([1024])\n",
      "transformer.h.12.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.12.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.12.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.12.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.12.ln_2.weight torch.Size([1024])\n",
      "transformer.h.12.ln_2.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.12.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.13.ln_1.weight torch.Size([1024])\n",
      "transformer.h.13.ln_1.bias torch.Size([1024])\n",
      "transformer.h.13.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.13.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.13.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.13.ln_2.weight torch.Size([1024])\n",
      "transformer.h.13.ln_2.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.13.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.13.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.13.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.14.ln_1.weight torch.Size([1024])\n",
      "transformer.h.14.ln_1.bias torch.Size([1024])\n",
      "transformer.h.14.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.14.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.14.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.14.ln_2.weight torch.Size([1024])\n",
      "transformer.h.14.ln_2.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.14.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.14.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.14.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.15.ln_1.weight torch.Size([1024])\n",
      "transformer.h.15.ln_1.bias torch.Size([1024])\n",
      "transformer.h.15.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.15.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.15.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.15.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.15.ln_2.weight torch.Size([1024])\n",
      "transformer.h.15.ln_2.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.15.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.15.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.15.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.16.ln_1.weight torch.Size([1024])\n",
      "transformer.h.16.ln_1.bias torch.Size([1024])\n",
      "transformer.h.16.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.16.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.16.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.16.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.16.ln_2.weight torch.Size([1024])\n",
      "transformer.h.16.ln_2.bias torch.Size([1024])\n",
      "transformer.h.16.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.16.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.16.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.16.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.17.ln_1.weight torch.Size([1024])\n",
      "transformer.h.17.ln_1.bias torch.Size([1024])\n",
      "transformer.h.17.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.17.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.17.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.17.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.17.ln_2.weight torch.Size([1024])\n",
      "transformer.h.17.ln_2.bias torch.Size([1024])\n",
      "transformer.h.17.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.17.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.17.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.17.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.18.ln_1.weight torch.Size([1024])\n",
      "transformer.h.18.ln_1.bias torch.Size([1024])\n",
      "transformer.h.18.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.18.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.18.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.18.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.18.ln_2.weight torch.Size([1024])\n",
      "transformer.h.18.ln_2.bias torch.Size([1024])\n",
      "transformer.h.18.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.18.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.18.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.18.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.19.ln_1.weight torch.Size([1024])\n",
      "transformer.h.19.ln_1.bias torch.Size([1024])\n",
      "transformer.h.19.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.19.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.19.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.19.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.19.ln_2.weight torch.Size([1024])\n",
      "transformer.h.19.ln_2.bias torch.Size([1024])\n",
      "transformer.h.19.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.19.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.19.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.19.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.20.ln_1.weight torch.Size([1024])\n",
      "transformer.h.20.ln_1.bias torch.Size([1024])\n",
      "transformer.h.20.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.20.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.20.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.20.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.20.ln_2.weight torch.Size([1024])\n",
      "transformer.h.20.ln_2.bias torch.Size([1024])\n",
      "transformer.h.20.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.20.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.20.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.20.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.21.ln_1.weight torch.Size([1024])\n",
      "transformer.h.21.ln_1.bias torch.Size([1024])\n",
      "transformer.h.21.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.21.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.21.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.21.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.21.ln_2.weight torch.Size([1024])\n",
      "transformer.h.21.ln_2.bias torch.Size([1024])\n",
      "transformer.h.21.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.21.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.21.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.21.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.22.ln_1.weight torch.Size([1024])\n",
      "transformer.h.22.ln_1.bias torch.Size([1024])\n",
      "transformer.h.22.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.22.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.22.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.22.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.22.ln_2.weight torch.Size([1024])\n",
      "transformer.h.22.ln_2.bias torch.Size([1024])\n",
      "transformer.h.22.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.22.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.22.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.22.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.h.23.ln_1.weight torch.Size([1024])\n",
      "transformer.h.23.ln_1.bias torch.Size([1024])\n",
      "transformer.h.23.attn.c_attn.weight torch.Size([1024, 3072])\n",
      "transformer.h.23.attn.c_attn.bias torch.Size([3072])\n",
      "transformer.h.23.attn.c_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.23.attn.c_proj.bias torch.Size([1024])\n",
      "transformer.h.23.ln_2.weight torch.Size([1024])\n",
      "transformer.h.23.ln_2.bias torch.Size([1024])\n",
      "transformer.h.23.mlp.c_fc.weight torch.Size([1024, 4096])\n",
      "transformer.h.23.mlp.c_fc.bias torch.Size([4096])\n",
      "transformer.h.23.mlp.c_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.23.mlp.c_proj.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.weight torch.Size([50257, 1024])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model.state_dict()\n",
    "for k, v in sd_hf.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hooks import *\n",
    "remove_all_forward_hooks(model)\n",
    "register_all_forward_hooks(model)\n",
    "\n",
    "batch_size = 16\n",
    "total_samples = 1024\n",
    "num_batches = total_samples // batch_size\n",
    "\n",
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        outputs = model(**inputs)\n",
    "compute_importance_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer LayerNorm: Ptransformer.h.0.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.0.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.0.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.0.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.1.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.1.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.1.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.1.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.2.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.2.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.2.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.2.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.3.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.3.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.3.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.3.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.4.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.4.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.4.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.4.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.5.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.5.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.5.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.5.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.6.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.6.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.6.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.6.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.7.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.7.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.7.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.7.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.8.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.8.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.8.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.8.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.9.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.9.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.9.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.9.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.10.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.10.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.10.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.10.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.11.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.11.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.11.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.11.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.12.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.12.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.12.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.12.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.13.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.13.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.13.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.13.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.14.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.14.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.14.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.14.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.15.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.15.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.15.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.15.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.16.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.16.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.16.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.16.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.17.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.17.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.17.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.17.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.18.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.18.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.18.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.18.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.19.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.19.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.19.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.19.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.20.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.20.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.20.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.20.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.21.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.21.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.21.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.21.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.22.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.22.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.22.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.22.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.h.23.ln_1 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.23.attn.c_proj : importance scores: torch.Size([16])\n",
      "Layer LayerNorm: Ptransformer.h.23.ln_2 : importance scores: torch.Size([1024])\n",
      "Layer Conv1D: Ptransformer.h.23.mlp.c_fc : importance scores: torch.Size([4096])\n",
      "Layer LayerNorm: Ptransformer.ln_f : importance scores: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "# Print importance scores for each registered module\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"importance_scores\"):\n",
    "        print(f\"Layer {module.__class__.__name__}: P{name} : importance scores:\", module.importance_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for module in model.modules():\n",
    "#     if hasattr(module, \"importance_buffer\") and module.importance_buffer:\n",
    "#         # print(module.importance_buffer)\n",
    "#         print(module.__class__.__name__)\n",
    "#         print(len(module.importance_buffer))\n",
    "#         print(module.importance_buffer[0].shape)\n",
    "#         print('============')\n",
    "        \n",
    "#         print([module.importance_buffer[i].shape for i in range(len(module.importance_buffer))])\n",
    "#         print('============')\n",
    "#         i += 1\n",
    "#         if i>10:\n",
    "#             break\n",
    "#         # all_outputs = torch.cat(module.importance_buffer, dim=0)  # Concatenate over batch dimension\n",
    "\n",
    "#         # # Compute norm-based importance\n",
    "#         # importance = all_outputs.norm(p=2, dim=0).mean(dim=0)\n",
    "\n",
    "#         # module.importance_scores = importance\n",
    "#         # del module.importance_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pruning' from '/users/eleves-a/2021/ahmad.rammal/Desktop/pruned_GPT/pruning.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pruning \n",
    "import pruning_utils\n",
    "import importlib\n",
    "importlib.reload(pruning_utils) \n",
    "importlib.reload(pruning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning.prune_mlp(model, 2048)\n",
    "pruning.prune_heads(model, 12)\n",
    "pruning.prune_embeddings(model, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Total Parameters: 171,720,960\n"
     ]
    }
   ],
   "source": [
    "from pruning_utils import *\n",
    "model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The little girl was Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope Telescope\n"
     ]
    }
   ],
   "source": [
    "# Sample prompt\n",
    "prompt = \"The little girl was\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "remove_all_forward_hooks(model)\n",
    "\n",
    "# Generate output text\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=100,  # Adjust length as needed\n",
    "        temperature=0.7,  # Adjust for diversity (lower = more deterministic)\n",
    "        top_k=50,  # Consider top-k sampling\n",
    "        top_p=0.95,  # Nucleus sampling\n",
    "        do_sample=True  # Enables sampling instead of greedy decoding\n",
    "    )\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=2048, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=2048)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", trust_remote_code=True)\n",
    "\n",
    "# # Check the size\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"A magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\\n\\nWhat it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\\n\\nBut in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\\n\\nNo law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\\n\\nBut the question of whether to publish it in the country where Hitler plotted his empire has lost some of its edge in the Google era, when a complete German-language copy of the book pops up as the second result on the local version of the search engine.\\n\\n“To say this is a very dangerous book, we must ban it, this is ridiculous,” said Wolfgang Wippermann, a professor of modern history at the Free University of Berlin. “Maybe it was necessary once, but now it’s over, it makes no sense. You can find it so easily.”\\n\\nThe publisher of the excerpts, London-based Albertus, has said it will appeal the Bavarian government’s injunction. In 2009, the publisher beat charges of copyright violation and the illegal use of Nazi symbols after the Bavarian government seized reprinted copies of the Nazi Party’s in-house newspaper.\\n\\nThe attempt to publish portions of “Mein Kampf” on Thursday was scuttled at the last moment, although the publisher, ready to capitalize on the publicity, had printed two versions of the pamphlet. The version propped on top of a heap of celebrity magazines at a newsstand in Berlin’s central Friedrichstrasse station was a slender, blue, 16-page leaflet that has historical commentary in one column and an image of blurred text stamped with “Unreadable” in the other, accompanied by two reproductions of Nazi-era newspapers.\\n\\n“Mein Kampf” “is an awful book, and the whole thinking is absolutely not ours, but we have another view on it regarding the idea of packing it away. This idea is just naive,” said Alexander Luckow, a spokesman for the publisher. “In a free country, you need to discuss these very bad parts of German history.”\\n\\nStill, he said, there are limits, and using Hitler’s words as inspiration, not as historical artifact, is where it crosses the line.\\n\\n“The danger is allowing right-wing people to sell it in bookshops with their modern commentary,” he said. “This is forbidden and it’s good . . . not only in Germany, this should be equal in other countries in Europe. Anti-Semitism is not confined to Germany. You look and it’s all around Europe, dating back to the Middle Ages.”\\n\\nThe debate will soon be over, whether or not the latest excerpts make it to newsstands. German law extends copyright 70 years after the author’s death; after 2015, “Mein Kampf” will be fair game. Some in Bavaria’s government worry that neo-Nazis will publish their own version of the book shortly thereafter, and to counter that, they are encouraging a scholarly edition. A group of historians is preparing it.\\n\\nGermany’s Jewish organizations have approached the publication with mixed emotions, sensitive that their country still has problems with neo-Nazis and anti-Semitism. The German government released a study this week that found that one in five Germans has anti-Semitic attitudes. And a neo-Nazi ring that has been linked to at least nine killings before it was shut down in November shocked Germans who thought they had done a thorough job working through their past.\\n\\n“I do very well without any publishing of ‘Mein Kampf,’ ” said Dieter Graumann, the head of the Central Council of Jews in Germany. “In a few years, it will be free, and I have every trust in the democratic maturity of the German people. . . . But for the moment, I am glad it is not.”\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenization function with truncation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = sum(len(tokenized_datasets['train'][i]['input_ids']) for i in range(10000))\n",
    "# for i in range(len(tokenized_datasets)):\n",
    "#     if len(tokenized_datasets['train']['input_ids'][i])>1024:\n",
    "#         print(i)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/ahmad.rammal/miniconda3/envs/colab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPT2ChunkedDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 1. Concatenate all texts into one long string\n",
    "        full_text = \" \".join(texts)\n",
    "        \n",
    "        # 2. Tokenize the entire text corpus\n",
    "        tokenized = tokenizer(full_text, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # 3. Split into fixed-length chunks of `max_length`\n",
    "        self.chunks = [tokenized[i:i + max_length] for i in range(0, len(tokenized), max_length)]\n",
    "        \n",
    "        # 4. Drop the last chunk if it's too short (optional)\n",
    "        if len(self.chunks[-1]) < max_length:\n",
    "            self.chunks.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.chunks[idx]}\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11241954 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", trust_remote_code=True)\n",
    "\n",
    "# Example calibration text dataset\n",
    "# calib_texts = [\n",
    "#     \"This is the first calibration text. It is meant to help with quantization.\",\n",
    "#     \"Another example of calibration data. It should be long enough to form multiple chunks.\",\n",
    "#     \"We concatenate all text, tokenize it, and split it into 1024-token chunks for GPT-2.\"\n",
    "# ]\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "calib_dataset = GPT2ChunkedDataset(dataset['train']['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10978"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calib_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dataloader = DataLoader(calib_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calib_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1212,   318,   262,   717, 36537,  2420,    13,   632,   318,  4001])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calib_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
